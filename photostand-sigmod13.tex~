\documentclass{sig-alternate}

\usepackage{graphicx}
\usepackage{subfig}
\usepackage{url}
\usepackage{verbatim}
\usepackage{xspace}

%\DeclareCaptionType{copyrightbox}

\newcommand{\eg}{e.g.\xspace}
\newcommand{\ie}{i.e.\xspace}
\newcommand{\etc}{etc.\xspace}
\newcommand{\etal}{et al.\xspace}
\newcommand{\vect}[1]{\ensuremath{\overrightarrow{#1}}}
\newcommand{\tm}[1]{\emph{#1}}

\newcommand{\namesep}{\hspace{1cm}}

\renewcommand{\baselinestretch}{0.933}



% MDL 15 Nov 2011:  Set PDF metadata.
\special{! /pdfmark where
  {pop} {userdict /pdfmark /cleartomark load put} ifelse [
    /Author (Hanan Samet, Marco D. Adelfio, Brendan C. Fruin, Michael D. Lieberman, Jagan Sankaranarayanan)
    /Title (PhotoStand: A Map Query Interface for a Database of News Photos)
    /Keywords (PhotoStand, NewsStand, Map query interface, Smartphone app)
    /DOCINFO pdfmark}

\begin{document}

\title{PhotoStand: A Map Query Interface for a Database of News
Photos\titlenote{This work was supported in part by the National
Science Foundation under Grants IIS-10-18475, IIS-09-48548,
IIS-08-12377, CCF-08-30618, and IIS-07-13501.}}

\numberofauthors{1}
\author{
\alignauthor
\begin{tabular}{c@{\namesep}c@{\namesep}c}
Hanan Samet & Marco D. Adelfio & Brendan C. Fruin\\
\end{tabular}\\
\begin{tabular}{c@{\namesep}c}
Michael D. Lieberman & Jagan Sankaranarayanan
\end{tabular}\\
\affaddr{Center for Automation Research, Institute for Advanced Studies,}\\
\affaddr{Department of Computer Science, University of Maryland}\\
\affaddr{College Park, MD  20742 USA}
\email{\{hjs, marco, brendan, codepoet, jagan\}@cs.umd.edu}
}

\maketitle

\begin{abstract}

PhotoStand enables the use of a map query interface for the retrieval
of news photos that are associated with news articles that are in turn
associated with the principal locations that they mention collected as
a result of monitoring the output of over 10,000 RSS news feeds, made
available within minutes of publication, and stored in a PostgreSQL
database.  The news photos are ranked according to their relevance to
the clusters of news articles that are associated with the locations at
which they are displayed.  This work differs from traditional work in
this field as the associated locations and topics (by virtue of the
cluster with which the articles containing the news photos are
associated) are generated automatically without any human intervention
such as tagging, and that the photos are retrieved by location instead
of just by keyword as is the case for many existing systems.
In addition, the clusters provide a filtering step for the detection
of duplicate and near-duplicate news photos.


\end{abstract}

\vspace{-8pt}

\section{Introduction}
\label{sec-introduction}

A demonstration is presented of the functioning of the PhotoStand
system (see also the related NewsStand~\cite{Lieb12b,Same11b,Same11,Teit08},
and STEWARD~\cite{Lieb07} systems).
PhotoStand is an example application of a general framework being
developed at the University of Maryland at College Park for retrieving
multimedia data (\eg, text, images, videos) using a map query
interface 
% MDL 15 Nov 2011:  Removed for now
(see also the
related systems QUILT~\cite{Shaf90} and the SAND
Browser~\cite{Same03}) from a database of news articles, photos, and
videose (\ie, by location in real-time which differentiates it from Google where static
photos are retrieved which rely on human geotagging).
% HJS 11-12-12:  Brendan - please provide some more substantive
%                differentiation from Google.
% BCF - added that our system is real-time and does not rely on human geotagging
The photos are associated with
news articles~\cite{Sank10}
collected by monitoring the output of over
10,000 RSS news feeds and made available for map-based retrieval
within minutes of publication.
These feeds are processed by the NewsStand system which
constantly polls them, downloads the new articles that they contain,
performs a variety of tasks on them, and stores the results in a
PostgreSQL database.

The three major processing modules of NewsStand are its \tm{cleaner}
module, which extracts the text, images, and videos, as well as
discards irrelevant objects in the feed; its \tm{geotagger}, which extracts
locations mentioned in the articles, enabling them
to be accessed by spatial queries such as a window query or a simple
point location query; and its \tm{clusterer}~\cite{Teit10}, which groups
articles about the same topic.
A key to to the NewsStand database system is its pipe server which
coordinates its processing modules by assigning batches of articles to  
them.
NewsStand's user interface enables the retrieval of clusters of news
articles for display using its map user interface by executing what we
term {\em top-k window queries}, which have both location-based and
feature-based variants~\cite{Aref90}.
At present, NewsStand handles about 50K articles per day and has a large
underlying database of articles currently containing about 300GB of
data.

The PhotoStand demo is related to the TweetPhoto
demonstration~\cite{Frui12} in the sense that PhotoStand uses photos
from news articles stored in the NewsStand system, while TweetPhoto
% BCF 11-13-12 - added "system" after TwitterStand
uses photos from news tweets stored in the TwitterStand~\cite{Sank09c} system.
In addition, the PhotoStand demo demonstrates the database querying
capability of NewsStand as well as its capability to do similarity
searching for news photos where the first step in the similarity
detection process is based on the text associated with the photos,
while the second step involves use of the actual image features
such as texture, color, etc. to enable detecting of duplicates 
% BCF 11-13-12 - changed "as well as" to and to get space
and 
near duplicates, thereby avoiding the combinatorial
complexity of comparing every photo with every other photo.

The rest of this paper is organized as follows.
Section~\ref{sec-related-work} discusses related work.
Section~\ref{sec-document-processing} indicates how the news articles (and
consequently the news photos) are clustered, as well as how the
captions are identified and extracted.
Section~\ref{sec-demo-scenario} describes the demo scenario and some
of the underlying interaction with the database, while
concluding remarks are given in
Section~\ref{sec-concluding-remarks}.

\vspace{-12pt}

\section{Related Work}
\label{sec-related-work}

Most of the work in associating geographic locations with images has
dealt with images that correspond to photos and involve tags generated
by humans (often just the dateline of the associated article in the
case of news photos) or by a GPS device that is built into the camera. 
Some examples include Flickr images.
One problem is that the user generated tags are not always sufficient
to generate precise latitude-longitude coordinate values which means
that they require additional human intervention to identify the
location although gazetteers do help.
In our work, we limit ourselves to photos that accompany news articles
and use the vector space model of the contents of article
documents to help us identify similar photos. These feature vectors
are often sufficient to describe both the documents and the images that
the documents contain.
In fact, the feature vectors can be viewed as tags, although no humans
are involved in their creation.

The idea of geotagging images based on image tags (or captions) or
using the image's GPS coordinates has been extensively
explored~\cite{Aher07,Cran09,Over09,Serd09,Wein08}, often using a
collection of images uploaded to Flickr.
Serdyukov \etal~\cite{Serd09} rely solely on Flickr image tags to
geotag images and to place the geotagged images on a map at varying
levels of granularity.
While similar to PhotoStand in its representation and extraction of
location information, this method of geotagging may return ambiguous
results when the image tag lacks precision.
Crandall \etal~\cite{Cran09} propose an improvement over a tag-only
grouping system by incorporating the GPS coordinates.
This approach is not feasible for a system such as PhotoStand, which
queries thousands of news sources that may not geotag their images.

For another approach, see the Google Maps'\cite{Goog12} photos layer
which shows a world map overlaid with user submitted images at the
location where the images were taken.  
Similar to PhotoStand's user interface, the set of displayed photos
changes as the viewing window changes.
However, the photos layer of Google Maps requires image locations to
be tagged for each photo while PhotoStand automatically 
geotags an image using the article where the image was found.
PhotoStand also focuses on images pertaining to the news which often results
in the map's photos being updated whereas photos in Google Maps are of 
geographical landmarks which update less often.  Another example is
the image hosting site Flickr which has a map \cite{Flic12} that
displays markers for the top geotagged locations in the world.
% HJS 11-12-12:  Brendan - I don't understand the following sentence.
Unlike PhotoStand, Flickr's map does not update to display new images
when the viewing window changes.
% BCF 11-13-12:  Removed "such as when zooming in."
% HJS 11-12-12:  Brendan - I think what you really want to say is that
%                when you zoom in, you don't get more images.
%                However, I want to ake sure that this is true before
%                saying it.  Please verify for Flickr and also for Google.
This means that locations which have images, but are not in the top geotagged
image locations are not shown.
% HJS 11-12-12:  The following seems repetitive.  So omit!
% Flickr's map also relies on GPS tagged
% information or manually entered user information. Kinsella
% \etal~\cite{Kins11} geotags  Twitter data for varying levels of
% granularity, but focuses on where the user was when they tweeted as
% opposed to the content of the message that was tweeted as is the case
% in TweetPhoto.
 
% BCF 11-13-12: Commented out
\begin{comment}
A prevalent issue in using image tags is dealing with ambiguity.
Many researchers have used
Wikipedia in order to disambiguate images with
ambiguous tags (\eg,~\cite{Busc06,Klie08,Over09}).
Buscaldi \etal~\cite{Busc06} applied a comparable algorithm to the
algorithm employed in PhotoStand in order to score each image in
relation to a location using Wikipedia.
However, all words were 
% BCF 11-13-12: "treated with the same weight" to weighted the same
weighted the same
when searching
for a location, while we modify the algorithm to give weighting to
words with higher frequencies.
Overell \etal~\cite{Over09} used a library of Flickr images along with
a Wikipedia dump in order to categorize tags.
The Wikipedia articles extracted would only have the news events up
until the time of the 
% BCF 11-13-12:  "Wikipedia dump" to snapshot
snapshot, while PhotoStand relies on
% BCF 11-13-12: took out "dynamic,"
 real-time ambiguity checking.
\end{comment}

\begin{comment}
% HJS 11-12-12:  Keep out for the present
In addition, much work has addressed the
clustering and scoring of images to produce a relative rank amongst
images in a collection (\eg,~\cite{Epsh07,Jaff06,Over09}).
Epshtein \etal~\cite{Epsh07} clustered a set of images based on the
location that the picture was taken and the viewing angle.
This methodology often groups images of the same landmark or location
which would not allow for related but distinct news images,
as in our application, to be
clustered together.
Jaffe \etal~\cite{Jaff06} proposed a solution to the issue of
clustering and ranking images using the image location, image tags,
the photographer, and other auxiliary metadata.
Unfortunately, this relies on extra information that is often not
present in images from news articles.
More generally, the main drawback of all the above methods is their
reliance on external data sources, while PhotoStand is able to perform
all clustering, ranking and geotagging operations with its own data.
\end{comment}


% BRENDAN:  Please add text that you and Mike came up with for the
% related work section.

\vspace{-10pt}

\section{Document Preprocessing}
\label{sec-document-processing}

We use the {\em vector space model}
% MDL 15 Nov 2011:  Unneeded citation
%~\cite{Salt75}
of documents, often used in text mining
and information retrieval.
This model represents a text document
as a {\em term feature vector}
in a $d$-dimensional space, where $d$ is the number of
distinct terms in every document in a corpus.

Upon receiving a new article to be clustered,
we first normalize the article's content
by {\em stemming}
% MDL 15 Nov 2011:  Unneeded citation
%\cite{Port80}
input terms and removing punctuation
and other extraneous characters.
Next, we extract the article's term feature vector
by computing the well-known
{\em Term Frequency-Inverse Document Frequency} (TF-IDF)
% MDL 28 Mar 2012:  Citation not needed, obvious.
%~\cite{Salt88}
score for each term in the article.
This score emphasizes terms that are frequent in a particular
document and infrequent in a large corpus $D$ of documents.
% MDL 15 Nov 2011:  Too much detail
\begin{comment}
The TF-IDF score for a term $t_i$ in article $d_j$ is
    $$\mbox{TF-IDF}_{i,j} = \frac{n_{i,j}}{n_j}
        \cdot \log \frac{|D|}{O_i}$$
where $n_{i,j}$ is the number of
occurrences of $t_i$ in $d_j$,
$n_j$ is the number of terms in $d_j$,
and $O_i$ is the number of articles in $D$ that contain $t_i$.
\end{comment}

Our clustering algorithm is a variant of leader-follower
clustering
% MDL 15 Nov 2011:  Unneeded citation
%~\cite{Duda00}
that permits online clustering in the term
vector space.
For each cluster, we maintain a {\em term centroid} and {\em time
centroid}, corresponding to the means of all term feature vectors and
publication times of articles in the cluster, respectively.
To cluster a new article $a$, we check whether there exists a cluster
where the distance, a variant of the cosine similarity measure,
from its term and time centroids to $a$ is less
than a fixed cutoff distance $\epsilon$.
If one or more candidate clusters exist, $a$ is added to the closest
such cluster, and the cluster's centroids are updated.
Otherwise, a new cluster containing only $a$ is created.

\begin{comment}
% HJS 11-12-12:  No need as mentioned above
We use a variant of the {\em cosine similarity measure}
% MDL 15 Nov 2011:  Unneeded citation
%~\cite{Stei00}
for computing term distances between the new article
and candidate clusters.
The term cosine similarity measure for a article $a$ and cluster $c$
is defined as
    $$\delta(a,c)
        = \frac{\vect{TFV_a} \bullet \vect{TFV_c}}
                {||\vect{TFV_a}|| \, ||\vect{TFV_c}||}$$
where $\vect{TFV_k}$ is the term feature vector of $k$.

\end{comment}

% \section{Caption Extraction}
% \label{sec-caption-extraction}

In addition to identifying the article in the web page of the HTML RSS
news feeds, we also need to find and extract the images, which usually
have captions which are textual descriptions of the images that
describe the scene that they capture.
This is done by processing the HTML tags. 
Observe that we may not be able to identify a caption for each of the
images in a news article, but from our experience, we are able to do
so for a large percentage of them, although some may correspond to ads
or other irrelevant objects.
Even though the captions of images are usually not very descriptive due
to their succinctness, they still capture their content in the sense
that they have terms in common with the text, and hence the captions
and text are said to be {\em similar}.

We examine every image in the HTML page.
If we can visualize the HTML as a tree structure, and the image as a
node in the tree, then the idea is to look at the children nodes and a
few ancestor nodes to try to collect enough text which would serve as
the caption of the image.
Sometimes the image may have a {\sc title} field associated with it,
in which case it forms the caption of the image.
In some cases there may be an {\sc alt} field, which can also serve as
a caption.
We also look for configurations where the image is embedded in a {\sc
div} element, in which case we use any text found within the  {\sc
div} element.
Our algorithms use several configurations such as nested {\sc div} and
{\sc table} structures, or combinations of them.
Note that the caption is usually not very long, which means
that we can simply discard any text if it is too long.
In addition, note that we require that the image have a minimum
size and an aspect ratio greater than 1.5, as is typical
with images accompanying news articles.

Once we have a caption for the image, we try to match the terms
(\ie, words) it contains with the
% HJS 03-27-12:  I think that the following should be ``document's
%                term feature vector'' as we are just working with the
%                document with which the photo is associated.
% MDL 28 Mar 2012:  No- we compare it against the cluster's feature
%   vector.  We refer to it as the term centroid above.  I put
%   it here.
%document's term feature vector
cluster's term centroid to see how many keywords from the cluster are
found in the caption. 
For example, if the feature vector of the document contains
``Obama'', ``Bohner'', ``Debt'', and ``Congress'', then we
would expect that one or more of these ``features'' be present in the
caption text.
If not, then we simply discard the image.
Once the image has been extracted, we also record the caption text and
the
% HJS 03-27-12:  I think that the following should be ``document's
%                term feature vector'' as we are just working with the
%                document with which the photo is associated.
% MDL 28 Mar 2012:  Changed as above
%document's term feature vector
cluster term centroid
in the database.

\vspace{-10pt}

\section{Duplicate and Near Duplicate Image Detection}
\label{sec-duplicate-images}

In order to efficiently store and process large collections of
candidate near-duplicate images we need a representation for each
image which is extremely compressed and efficient to compute. It is
% BCF 11-13-12: removed 'also'
crucial that the representation be resistant to changes in scale,
saturation, hue, contrast, compression, and cropping. 
% BCF 11-13-12: removed "We devised" 
A hierarchical color histogram 
% BCF 11-13-12 "which" is used to 
is used to follow the pattern of an image
pyramid with three levels. 

\begin{figure}[ht]
\vspace{-3mm}
\centering
\label{pyramid}
\includegraphics[width=3in]{figs/wikileaksPyramid.jpg.eps}
\vspace{-4mm}
% Image created using AP photo source
\caption{Illustration of the pyramid structure and color channel segments which are used to compute the hierarchical color histograms.}
\vspace{-2mm}
\end{figure}

Thus, we utilize a global histogram from the first level, a histogram
for each quadrant from the second level, and
% BCF 11-13-12: "finally" removed
sixteen
histograms from the third level. This structure allows us to encode
and compare global and local features of the images, preserving
spatial information. We use 
% BCF 11-13-12: "the" removed
less memory to encode the
histograms at lower levels, ensuring that 
% BCF 11-13-12: "the" removed
the information from
higher levels receives appropriate weight.  

%In order 
To encode an image's color information, we first convert
images into the Lab color space since it approximates human visual
perception. After a histogram is calculated for a channel of a segment
in the image pyramid, the bins of the histogram are shifted to
minimize its vector representation. 
This process addresses
changes in hue found in news images.  
For images in
grayscale, we only encode their lightness information, so when
comparing vectors of a grayscale and color images we only consider the
lightness components. 

The histograms are concatenated in a feature vector, and depending on
the precision needed for the application, the number of histogram bins
and data used to represent them can be 
% BCF 11-13-12: "increased or decreased" replaced by "modified"
modified. We
store each bin as a single byte, and our entire feature vector is 512
bytes. The similarity between two images can be computed as the
euclidean distance between their vectors. This allows us to group or
retrieve duplicates using common clustering or retrieval techniques. 

Examples of image near-duplicates detected by our approach are shown
in Figure 2. 
% BCF \ref{eg:newsstand}. 
It is important to note that most news 
images on the web undergo limited transformations or alterations. In particular, 
this method is not robust to occlusions or significant cropping. These
transformations can dramatically affect the intensity and color layout
of the image. In Figure 
%\ref{eg:threethesame}, 
3,
the duplicate images
all share similar color and intensity structure despite other
differences in the images, so these duplicates were detected. However,
if an image is cropped such that a primary color element is removed,
then the histogram is fundamentally changed and the duplicate may not
be detected.  
% BCF 11-13-12: Hanan - we no longer have these images
%For example, In Figure %\ref{missed} TODO
%we see that the right image has been cropped such that the intensity layout of the image has changed dramatically, and these two images were not considered near-duplicates by our method. In applications that need these types of image changes, partial-duplicate image retrieval and sub-image similarity techniques might be better suited approaches. However, for NewsStand and similar systems which focus on near-duplicate image detection where these transformations are rare, the color histogram based approach is very efficient, simple, and effective.

\begin{figure}[ht]
\centering
\label{eg:newsstand}
\includegraphics[height=.8in]{figs/time1.jpg.eps} %Barton Silverman The New York Times
\includegraphics[height=.8in]{figs/time2.jpg.eps}
\includegraphics[height=.8in]{figs/crop1a.jpg.eps} %Mohammed Zaatari AP
\includegraphics[height=.8in]{figs/crop1b.jpg.eps}
%\includegraphics[height=1in]{figs/hue1a.jpg.eps} % AP
%\includegraphics[height=1in]{figs/hue1b.jpg.eps}
\includegraphics[height=.8in]{figs/bw1a.jpg.eps} % AP
\includegraphics[height=.8in]{figs/bw1b.jpg.eps}
\caption{Examples of near-duplicate images detected in real news sources. First Row: Similar time instances. Second Row: Different image croppings. 
 %Third Row: Changes in image brightness, contrast, and hue. 
	Third Row: Similar grayscale and color images.}
\vspace{-4mm}
\end{figure}

\begin{figure}[ht]
\centering
\label{eg:threethesame}
\includegraphics[width=1.0in]{figs/giffords2.jpg.eps} % Susan Walsh AP
\includegraphics[width=1.0in]{figs/giffords1.jpg.eps}
\includegraphics[width=1.0in]{figs/giffords3.jpg.eps}
\caption{%Near-duplicate images automatically detected by NewsStand}
Three examples of near-duplicate images automatically detected from unique news sources by our system.}
\vspace{-6mm}
\end{figure}

\begin{comment}
\begin{figure}[ht]
\centering
\label{missed}
\includegraphics[height=1.25in]{figs/missed2.jpg.eps} % Kathy Willens AP
\includegraphics[height=1.25in]{figs/missed1.jpg.eps}
\caption{An example of similar images that were not detected by the hierarchical color histogram.}
\vspace{-4mm}
\end{figure}
\end{comment}

\section{Demo Scenario}
\label{sec-demo-scenario}

The PhotoStand user interface consists of a map where 
% BCF 11-13-12: "miniature news photos" replaced by photo thumbnails
photo thumbnails
are displayed at the locations that are deemed most appropriate
to the clusters that contain the news articles with which the photos
are associated (\eg, Figure~\ref{fig-map}).
Initially, the map contains 
% BCF 11-13-12: "miniature news photos" replaced by photo thumbnails
photo thumbnails at locations
corresponding to those deemed most relevant in the $k$ most
representative clusters, where ``representative'' takes into account
the importance of the cluster's subject as measured by factors
such as currency, size and rate of growth of the cluster in
terms of velocity and acceleration rates, as well as a desire to have
a good spatial distribution in the area being displayed (\ie, the
viewing window)~\cite{Nuta12}.
A slider is 
% BCF 11-13-12: "present at the upper center of the map"
centered above the map whose
movement to the right (left) allows the maximum number of
different clusters for which news photos are displayed at their
representative locations to increase (decrease).
This corresponds to a location-based query where a spatial region in
the form of a location (specified by entering the location's name in the
``locate'' box in the upper right on the map) or region (\ie, the
viewing window) are the parameters, and the results are the
clusters and news photos associated with them.

A variant of the feature-based query can be executed by entering
a keyword in the ``search'' box at the lower left of the map.
% HJS:  The following is a rewrite and is a bit new.
The result is a set of $k$ 
% BCF miniature news photos
photo thumbnails whose captions contain 
which are displayed at the locations that are deemed
most appropriate to the clusters that contain the articles with which
the photos are associated.
An alternative, which we do not use at present, bases the selection of
the photos on the text of the associated articles rather than the
captions.
The $k$ 
% BCF miniature news photos 
photo thumbnails are ordered according to the importance
of the clusters that contain the articles with which the photos are
associated.
% HJS:  END OF NEW/REVISED TEXT
Once a search has been initiated, all subsequent searches are
restricted to the keyword.
However, the searches are also restricted to the displayed part of the
map (in other words, they are spatially restricted and are analogous
to a spatial join operation).
Users also have the option to restrict the sources of the articles
with which the images are associated, as well as the language in which
they are written.
This can be done by specifying the names of the sources (\eg,
``Washington Post''), the geographic regions in which they are
published (\eg, Ireland, UK), or the language in which they are
written (e.g., ``French'').

% HJS 03-27-12:  The following does not make sense as we are trying to
%                explain how we choose the locations and the photos.
%                Too complex for here and later we talk about scores
%                in choosing the representative photo.  So I suggest
%                to simply omit this paragraph.
% MDL 28 Mar 2012:  OK
\begin{comment}
As pointed out above, the identity of the $k$ locations for which the
% BCF miniature news photos 
photo thumbnails are displayed are a direct result of the choice
of the $k$ most representative clusters.
Once the clusters have been chosen, we obtain the representative news
photo
by examining all of the clusters associated with this location and
ranking the photos associated with their constituent articles in the
following manner. 
Each cluster has a set of cluster terms which are the words (up to a
maximum of 20) that occur most frequently in the text of the articles
associated with the cluster.
In addition, we record the frequency of their occurrence in these
clusters.
\end{comment}

As we pointed out before, the results of extracting, geocoding, and
clustering the news articles 
as well as the photos are stored in a relational database (PostgreSQL).
An entry for every relevant location (based 
on geotagging the article text) of each news story (\ie, clustered collection
of news articles) is stored in a table known as \emph{news\_map\_info}, which
is queried directly by the PhotoStand system.  Each entry also contains a
reference to a specific image, which is used to represent the associated news
story at that geographic location.  Specifically, each image is assigned a
relevance score, as described below, and then the highest scoring image is
selected as the representative image for that cluster at that location.
As we pointed out, PhotoStand's map query interface allows users to specify a geographic window of
interest, along with feature-based filters such as a textual search query, or a
category of news, which are translated into
parameters for a database query.  A sample query is shown here.

{\small
\begin{verbatim}
1:   SELECT story_name, image_source, image_title, 
2:          story_score, latitude, longitude
3:     FROM news_map_info
4:    WHERE latitude BETWEEN min_lat AND max_lat
5:      AND longitude BETWEEN min_lon AND max_lon
6:      AND story_keywords CONTAINS search_string
7:      AND topic IN search_topic_list
8: ORDER BY story_score DESC;
\end{verbatim}
}
%\end{quote}



\newcommand{\incfig}[2]{\includegraphics[#2]{figs/#1}\label{#1}}
% HJS 03-28-12:  Hanan commented Mike's figure handling in order to
% revert to Brendan's approach of just 6 figures in one column.
\begin{comment}

\begin{figure*}[t]
\centering
\subfloat[]{\incfig{fig-map}{width=0.44\columnwidth}}\hfill
%\subfloat[]{\incfig{fig-marker-selected}{width=0.44\columnwidth}}\hfill
%\subfloat[]{\incfig{fig-topics}{width=0.44\columnwidth}}\hfill
%\subfloat[]{\incfig{fig-topic-selected}{width=0.44\columnwidth}}\\
%\subfloat[]{\incfig{fig-all-images}{width=0.44\columnwidth}}\hfill
\subfloat[]{\incfig{fig-mark-dups}{width=0.44\columnwidth}}\hfill
%\subfloat[]{\incfig{fig-select-image}{width=0.44\columnwidth}}\hfill
%\subfloat[]{\incfig{fig-story}{width=0.44\columnwidth}}\\
% MDL 28 Mar 2012:  TEXT NEEDS TO BE UPDATED
\caption{Various interaction modes in PhotoStand's user interface, as
implemented on an iPhone/iPad Touch:
%\subref{fig-map} map view showing news photos and the
%principal locations of their corresponding clusters;
%\subref{fig-marker-selected} result of tapping on a
%location in PhotoStand, yielding the caption and location of a news
%photo at that location;
%\subref{fig-topics} FIXME;
%\subref{fig-topic-selected} FIXME;
%\subref{fig-all-images} grid of news photos associated with a
%location in ranked order;
\subref{fig-mark-dups} grid of news photos associated with
a location in ranked order where near duplicates are shown in gray
%\subref{fig-select-image} enlarged image and its
%caption;
%\subref{fig-story} article corresponding to a news
%photo.
\label{fig-photostand}}
\end{figure*}
\end{comment}

% MDL 28 Mar 2012:  Old figures using Brendan's naming convention
\begin{comment}
\begin{figure}
\centering
\subfloat[]{\incfig{fig-photostand-ui}{width=0.44\columnwidth}}\hfill
\subfloat[]{\incfig{fig-photostand-caption-and-location}{width=0.44\columnwidth}}\\
\subfloat[]{\incfig{fig-photostand-grid}{width=0.44\columnwidth}}\hfill
% \subfloat[]{\incfig{fig-photostand-duplicates}{width=0.44\columnwidth}}\\
\subfloat[]{\incfig{fig-photostand-image-and-caption}{width=0.44\columnwidth}}\hfill
\subfloat[]{\incfig{fig-photostand-article}{width=0.44\columnwidth}}\\
\subfloat[]{\incfig{fig-photostand-topics}{width=0.44\columnwidth}}\\
\caption{
%Various interaction modes in PhotoStand's user interface, as
%implemented on an iPhone/iPad Touch:
\subref{fig-photostand-ui} map view showing news photos and the
principal locations of their corresponding clusters;
\subref{fig-photostand-caption-and-location} result of tapping on a
location in PhotoStand, yielding the caption and location of a news
photo at that location;
\subref{fig-photostand-grid} grid of news photos associated with a
location in ranked order;
% HJS 03-28-12:  Removed dups and replaced by topics
% \subref{fig-photostand-duplicates} grid of news photos associated with
% a location where near duplicates are shown in gray;
\subref{fig-photostand-image-and-caption} enlarged image and its
caption;
\subref{fig-photostand-article} article corresponding to a news
photo.\label{fig-photostand}}
\subref{fig-photostand-topics} grid of news photos associated with a
location with one photo per cluster.\label{fig-photostand}}
\end{figure}
\end{comment}

% HJS 03-28-12:  Figures in three rows of two per row so all in one column
\begin{figure}
\centering
\subfloat[]{\incfig{fig-map}{width=0.39\columnwidth}}\hfill
%\subfloat[]{\incfig{fig-map-selected}{width=0.44\columnwidth}}\\
\subfloat[]{\incfig{fig-mark-dups}{width=0.39\columnwidth}}
%\subfloat[]{\incfig{fig-grid}{width=0.44\columnwidth}}\hfill
%\subfloat[]{\incfig{fig-tweet}{width=0.44\columnwidth}}\\
\caption{
%Various interaction modes in PhotoStand's user interface, as
%implemented on an iPhone/iPad Touch:
\subref{fig-map} Map view showing news photos and the
principal locations of their corresponding clusters
%\subref{fig-mark-dups} grid of news photos associated with Istanbul
%in ranked order where duplicate images are marked (grayed)
%\subref{fig-marker-selected} result of tapping on a
%location in PhotoStand, yielding the caption and location of a news
%photo at that location;
%\subref{fig-all-images} grid of news photos associated with a
%location in ranked order;
% HJS 03-28-12:  Removed dups and replaced by topics
 \subref{fig-mark-dups} Grid of news photos associated with
 a location in ranked order where near duplicates are shown in gray
%\subref{fig-select-image} enlarged image and its
%caption;
%\subref{fig-story} article corresponding to a news
%photo;
%\subref{fig-topics} grid of news photos associated with a
%location with one photo per cluster.
\label{fig-photostand}}
\vspace{-6mm}
\end{figure}

%Our initial implementation ranked an individual photo $p$ using the
%Jaccard index by computing the cardinality of the intersection of the
%set comprising the union of the cluster terms associated with the
%location and the set of words occurring in the caption and dividing
%this quantity by the cardinality of the union of the cluster terms
%associated with the location and the words in the caption.
%The images are ranked by the increasing order of these ratios.
%Unfortunately, using this approach finds that the denominators
%are almost identical and thus the values of the ratios are primarily
%dependent on the values of the numerators which do not have such a
%great variance or range.
%Another problem is that the frequencies of the cluster terms do not
%enter into play here as the rank is really just based on the number of
%cluster terms that the caption has in common with the cluster terms of
%the location.
%An alternative that overcomes these problems is to replace the count
%of the number of words that the caption has in common with the cluster
%terms in the articles by the frequencies of the cluster terms in the
%articles.

In scoring an image, we are looking to find the similarities between
the set of location keywords or cluster keywords and the set of the
image's caption words.
A traditional approach for finding the similarities between sets
\emph{A} and \emph{B} is to use the Jaccard Index which is defined as:
 $J(A,B) = \frac{|A \cap B|}{|A \cup B|}$.
Hence, the resultant index score ranges from 0 to 1, where 0 indicates
that the two sets have nothing in common while 1 indicates the sets
are the same.
Unfortunately, using this approach the frequencies of the keyword
terms do not enter into play as the rank is really just based on the
number of keywords that the caption has in common with the location
terms or cluster terms.
Using a variation of the Jaccard Index, we define the numerator to be
the sum of the cardinality of the intersection of the set comprising
the union of the terms associated with the locations or news cluster
and the set of words occurring in the caption and the frequencies
associated with each of the words in the intersecting set.
This quantity is divided by the sum of the cardinality of the union of
the terms associated with a location or news cluster and the words in
the caption and the total frequencies of all the terms to get an
\emph{image\_score}.
In order to also rank the image using its recency, the final
score is defined as $(image\_score * time\_factor) + days\_offset$
where $time\_factor$ is a constant that gives appropriate weight to
the score found using the modified Jaccard Index, and
\emph{days\_offset} is the number of days since the module was
started.
This score update guarantees both that recent images will have
a higher score, and that an image scoring highly using our modified
Jaccard Index will not always remain the highest scoring image for a
given location or cluster.

% MDL 28 Mar 2012:  TEXT NEEDS TO BE UPDATED
The above technique enables us to rank the photos thereby enabling us
to choose the most representative photo to be displayed on the map.
Users can also display the remaining photos in decreasing
order of their ranking.
This is achieved by the following two-step process.
A single click on the miniature news photo on the map reveals the name
of the location and a small text string corresponding to a partial
caption.
% BCF 11-13-12: (\eg, Figure~\ref{fig-marker-selected}).
A subsequent click on the rightward pointing arrow reveals a grid with
other 
% BCF miniature news photos
photo thumbnails associated with the location ranked in
decreasing order
% HJS 03-28-12:  Mike - I added the following phrase as we need to say
%                what is the actual order of decreasing relevance!  AGREE?
of cluster relevance,
% MDL 28 Mar 2012:  Added following relevance factors from NewsStand
%   paper
which is based in part on the number of distinct news sources and
several other factors.
% BCF (\eg, Figure~\ref{fig-all-images}).
In the grid, one can mark near duplicate images 
% BCF - don't use SIFT
%(detected using
%variants of the SIFT method~\cite{Lowe99}) 
as well as remove them
% HJS 03-28-12:  We don't show this in this version as no space!
(\eg, Figure~\ref{fig-mark-dups}).
Clicking on a photo enlarges it to take up a large
part of the display screen as well as shows its caption.
% BCF (\eg, Figure~\ref{fig-select-image}).
A double click yields the text of the associated news article. % (\eg,
% BCF Figure~\ref{fig-story}).

Users can also view the photos associated with a location
%according to
by their associated clusters (one photo per cluster), % as in
%Figure~\ref{fig-topics}),
ranked by just using cluster terms for the specific cluster and only
using their frequencies in the articles in the cluster.
Clusters are ranked in importance the same way as 
%NewsStand ranks them.
in NewsStand.

\vspace{-8pt}

\section{Conclusion}
\label{sec-concluding-remarks}

% PhotoStand enables querying news images by the
% locations relevant to the events that they describe.
Both app and Web
(\url{http://photostand.umiacs.umd.edu})
versions of Photostand exist, and a screencast demo can be seen at
% HJS 03-27-12:  Brendan - we need a url here and to do the demo.
% HJS 11-12-12:  Brendan - please make sure that the url is correct
\url{http://photostand.umiacs.umd.edu/demo}.
Future work includes the incorporation of more complex image
descriptors such as those based on SIFT~\cite{Lowe99} 
%and the
%methods for speeding it up~\cite{Brendan-refs}.
or the efficent open source ORB~\cite{Rubl11} algorithm in our image
duplicate detection.

\vspace{-6pt}

\bibliographystyle{abbrv}
\begin{small}
\small
\bibliography{photostand}
\end{small}

\end{document}

